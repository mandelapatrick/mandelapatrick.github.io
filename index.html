<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Mandela Patrick</title>

  <meta name="author" content="Mandela Patrick">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
  <meta name="google-site-verification" content="aXa5D8Z7wUN7dmsP_E6zlmECaKiNsKiQma58LTBrj_U" />
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Mandela Patrick</name>
              </p>
              <p style="text-align:center"> Computer Vision | Machine Learning </p>


            </p>
            <p>I am a Machine Learning Scientist at <a href="https://www.pinatafarm.com/">Piñata Farms</a>, focused on machine learning for video understanding and recommendations. I completed my PhD with <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a> and <a href="https://www.robots.ox.ac.uk/~joao/">João Henriques</a> in the <a href="http://www.robots.ox.ac.uk/~vgg/">VGG group</a>, 
              at the University of Oxford. I was fortunate enough to be funded by a <a href="https://www.rhodeshouse.ox.ac.uk/scholarships/the-rhodes-scholarship/">Rhodes Scholarship</a> and <a href="https://aims.robots.ox.ac.uk/">AIMS</a>.
            </p>
            <p>
            Before that, I did my undergrad at <a href="https://college.harvard.edu/">Harvard College</a>, where I obtained a B.A in computer science.</a>
            </p>
            <p>
              <p style="text-align:center">
                <a href="mailto:mandelapatrick1ATMARKgmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=KOURpBsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/mandelapatrick/">Github</a>&nbsp/&nbsp
                <a href="https://twitter.com/mandelapatrick_">Twitter</a>&nbsp/&nbsp
                <a href="https://www.linkedin.com/in/mandelapatrick">LinkedIn</a>&nbsp/&nbsp
                <a href="media/Mandela_Patrick_Thesis_2021.pdf">Thesis</a>
                <!-- <a href="https://www.linkedin.com/in/mandelapatrick">LinkedIn</a>&nbsp/&nbsp -->
                <!-- <a href="data/Asano_CV.pdf">CV</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/mandela.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/mandela.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li>Started as Machine Learning Scientist at Piñata Farms!</li>
                  <li>Invited as a speaker at the <a href="https://sites.google.com/view/1st-ssll-workshop-iccv21">Share Stories and Lessons Learned (SSLL)</a> ICCV 2021 workshop.</li>
                  <li>My PhD thesis, <i>"Learning and interpreting deep representations from multi-modal data"</i> is publicly available on the <a href="https://ora.ox.ac.uk/objects/uuid:3a0721a0-025e-423c-b441-2d7af5d960da">Oxford Research Archive (ORA)</a>!</li>
                  <li>Our work Motionformer accepted as an Oral at NeurIPS 2021! <a href="https://github.com/facebookresearch/Motionformer">Code</a> </li>
                  <li>Passed my PhD! My examiners were <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a> and <a href="https://andrewowens.com/">Andrew Owens</a>.</li>
                  <li>Two papers (GDT and STiCA) accepted to ICCV'21! <a href="https://github.com/facebookresearch/GDT">Code</a> </li>
                  <li>Started at internship at Facebook AI, mentored by <a href="https://www.cs.cmu.edu/~fmetze/interACT/Home.html">Florian Metze</a>, <a href="https://feichtenhofer.github.io/">Christoph Feichtenhofer</a>, and <a href="https://imisra.github.io/">Ishan Misra</a>. </li>
                  <li>Our paper on multilingual multimodal video-text pretraining (MMP) got accepted at NAACL 2021! <a href="https://github.com/berniebear/Multi-HT100M">Code</a> </li>
                  <li>Our paper on video-text representation learning (SSB) got accepted as a Spotlight into ICLR 2021!</li>
                  <li>Our paper on Self-Labelling Videos (SeLaVi) got accepted at NeurIPS 2020! <a href="https://github.com/facebookresearch/selavi">Code</a> </li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, self-supervised learning and multi-modal learning.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/motionformer.png" alt="trajectory attention" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2106.05392">
                <papertitle>Keeping Your Eye On the Ball: Trajectory Attention in Video Transformers</papertitle>
              </a>
              <br>
              <strong>Mandela Patrick*</strong>, <a href="https://sites.google.com/view/djcampbell">Dylan Campbell*</a>, <a href="https://yukimasano.github.io/">Yuki M. Asano*</a>, <a href="https://imisra.github.io/">Ishan Misra</a>, <a href="https://www.cs.cmu.edu/~fmetze/interACT/Home.html">Florian Metze</a>, <a href="https://feichtenhofer.github.io/">Christoph Feichtenhofer</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>, <a href="http://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>
              <br>
              <em>NeurIPS</em>, 2021 &nbsp <font color=#FF8080><strong>(Oral)</strong></font>
              <br>
              <a href="https://github.com/facebookresearch/Motionformer">code | 
                <a href="media/motionformer-slides.pdf">slides</a> |
                <!-- <a href="media/GDT ICCV poster.pdf">poster</a> |  -->
                <a href="data/patrick2021motionformer.bib">bibtex</a> |
                <a href="https://recorder-v3.slideslive.com/?share=51221&s=74584e12-2332-43f6-a9bc-674a08bddd1a">talk</a>
              <p>We present <i>trajectory attention</i>, a drop-in self-attention block for video transformers that implicitly tracks space-time patches along motion paths. We set SOTA results on a number of action recognition datasets: Kinetics-400, Something-Something V2, and Epic-Kitchens.
            </td>
          </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/stica.png" alt="crops help training speed" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2103.10211">
                <papertitle>Space-Time Crop & Attend: Improving Cross-modal Video Representation Learning</papertitle>
              </a>
              <br>
              <strong>Mandela Patrick*</strong>, <a href="https://yukimasano.github.io/">Yuki M. Asano*</a>, <a href="http://www.cs.cmu.edu/~poyaoh/">Bernie Huang*</a>, <a href="https://imisra.github.io/">Ishan Misra</a>, <a href="https://www.cs.cmu.edu/~fmetze/interACT/Home.html">Florian Metze</a>, <a href="http://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICCV</em>, 2021 &nbsp </font>
              <br>
              <a href="https://github.com/facebookresearch/GDT">code | 
              <a href="media/StiCa_slides.pdf">slides</a> | 
              <a href="media/STiCA ICCV poster.pdf">poster</a> | 
              <a href="data/patrick2021spacetime.bib">bibtex</a>
              <p>We better leverage latent time and space for video representation learning by computing efficient multi-crops in embedding space and using a shallow transformer to model time. This yields SOTA performance and allows for training with longer videos.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gdt.png" alt="hierarchical transformations" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2003.04298">
                <papertitle>On Compositions of Transformations in Contrastive Self-Supervised Learning</papertitle>
              </a>
              <br>
              <strong>Mandela Patrick*</strong>, <a href="https://yukimasano.github.io/">Yuki M. Asano*</a>, <a href="https://scholar.google.com/citations?user=JdEJEicAAAAJ&hl=en&oi=ao">Polina Kuznetsova</a>, <a href="http://ruthcfong.github.io/">Ruth Fong</a>, <a href="http://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>, Geoffrey Zweig, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICCV</em>, 2021 &nbsp </font>
              <br>
              <a href="https://github.com/facebookresearch/GDT">code 
                | <a href="media/GDT-ICCV-2021-slides.pdf">slides</a> 
                | <a href="media/GDT ICCV poster.pdf">poster</a> 
                | <a href="data/patrick2020multimodal.bib">bibtex</a>
                | <a href="https://ai.facebook.com/blog/a-state-of-the-art-self-supervised-framework-for-video-understanding/">blog</a>
              <p>We give transformations the prominence they deserve by introducing a systematic framework suitable for contrastive learning. SOTA video representation learning by learning (in)variances systematically.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mmp.png" alt="Multilingual multimodal pretraining" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2103.08849">
                <papertitle>Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models</papertitle>
              </a>
              <br>
              <a href="http://www.cs.cmu.edu/~poyaoh/">Po-Yao Huang*</a>, <strong>Mandela Patrick*</strong>, <a href="https://junjiehu.github.io/">Junjie Hu</a>, <a href="http://www.phontron.com/">Graham Neubig</a>, <a href="https://www.cs.cmu.edu/~fmetze/interACT/Home.html">Florian Metze</a>, <a href="https://www.lti.cs.cmu.edu/AlexHauptmann">Alexander Hauptmann</a>
              <br>
              <em>NAACL</em>, 2021 &nbsp </font>
              <br>
              <a href="https://github.com/berniebear/Multi-HT100M">code | 
                <a href="media/NAACL_121_slides.pdf">slides</a> | 
                <a href="media/NAACL21_121b_poster.pdf">poster</a> | 
                <a href="data/huang2021mmp.bib">bibtex</a>
              <p>We develop a transformer model to learn contextualized multilingual multimodal embedddings and also release a new multilingual instructional video dataset (MultiHowTo100M) for pre-training. We apply this model in a zero-shot setting to retrieve videos with non-English queries, and outperform recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX; as well as in multilingual text-to-image search on Multi30K.
            </td>
          </tr>

           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/support_set.png" alt="Schematic of our method" width="160" height="160">
            </td>
              <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2010.02824">
                 <papertitle>Support-set bottlenecks for video-text representation learning</papertitle>
              </a>
              <br>
              <strong>Mandela Patrick*</strong>, <a href="http://www.cs.cmu.edu/~poyaoh/">Po-Yao Huang*</a>, <a href="https://yukimasano.github.io/">Yuki M. Asano*</a>, <a href="https://www.cs.cmu.edu/~fmetze/interACT/Home.html">Florian Metze</a>, <a href="https://www.lti.cs.cmu.edu/AlexHauptmann">Alexander Hauptmann</a>, <a href="http://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICLR</em>, 2021 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
              <br>
              <a href="media/SSB ICLR.pdf">slides</a> | 
              <a href="media/SSB_poster.pdf">poster</a> | 
              <a href="data/patrick2020supportset.bib">bibtex</a> | 
              <a href="https://slideslive.com/38953567/supportset-bottlenecks-for-videotext-representation-learning?ref=speaker-42650-latest">talk</a>
              <p>We use a generative objective to improve the instance discrimination limitations of contrastive learning to set new state-of-the-art results in text-to-video retrieval.
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/labelling_videos.gif" alt="clustered videos" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.robots.ox.ac.uk/~vgg/research/selavi/">
                <papertitle>Labelling unlabelled videos from scratch with multi-modal self-supervision</papertitle>
              </a>
              <br>
              <a href="https://yukimasano.github.io/">Yuki M. Asano*</a>, <strong>Mandela Patrick*</strong>, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>NeurIPS</em>, 2020
              <br>
              <a href="https://github.com/facebookresearch/selavi">code |</a>
              <a href="media/selavi_slides.pdf">slides</a> |
              <a href="media/SeLaVi_poster.pdf">poster</a> |
              <!-- <a href="https://www.robots.ox.ac.uk/~vgg/research/selavi/">homepage |</a>  -->
              <a href="data/asano2020labelling.bib">bibtex</a> | 
              <a href="https://slideslive.com/38955434/selavi-selflabelling-videos-without-any-annotations-from-scratch?ref=account-84503-latest"> talk</a>
              <p>Unsupervisedly clustering videos via self-supervision. We show clustering videos well does not come for free from good representations. Instead, we learn a multi-modal clustering function that treats the audio and visual-stream as augmentations.
            </td>
          </tr>
          <tr onmouseout="tripod_stop()" onmouseover="tripod_start()">
            <td width="25%">
              <div class="one">
              <div class="two" id="tripod_image"><img src="images/tripod_2.png" width="160" height="160" alt="tripod"></div>
              <img src="images/tripod_1.png" width="160" height="160" alt="tripod">
              </div>
              <script type="text/javascript">
              function tripod_start() {
              document.getElementById('tripod_image').style.opacity = "1";
              }
              function tripod_stop() {
              document.getElementById('tripod_image').style.opacity = "0";
              }
              tripod_stop()
              </script>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1910.08485">
                <papertitle>Understanding Deep Networks via Extremal Perturbations and Smooth Masks</papertitle>
              </a>
              <br>
              <a href="https://ruthcfong.github.io/">Ruth Fong*</a>, <strong>Mandela Patrick</strong>*, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICCV</em>, 2019 &nbsp <font color=#FF8080><strong>(Oral)</strong></font>
              <br>
              <a href="https://github.com/facebookresearch/TorchRay">code</a> |
              <a href="media/extremal_slides.pdf">slides</a> |
              <a href="http://ruthcfong.github.io/files/fong19_extremal_poster.pdf">poster</a> |
              <a href="data/fong2019extremal.bib" target="_blank">bibtex</a> |
              <a href="https://www.youtube.com/watch?v=qUu1076IMWo&t=632s"> talk</a>
              <p>We introduce extremal perturbations, an novel attribution method that highlights "where" a model is "looking." We improve upon Fong and Vedaldi, 2017 by separating out regularization on the size and smoothness of a perturbation mask from the attribution objective of learning a mask that maximally affects a model's output; we also extend our work to intermediate channel representations.
            </td>
          </tr>
          


        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Great template from <a href="https://jonbarron.info/"> Jon Barron</a>
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
